# 概念

索引：数据的存储单元，相当于表

文档：存储的数据，相当于一行记录

映射：文档写入索引前，会分析分词，过滤，一般由用户自定义规则。

文档类型：定义了文档的结构和数据类型

# 索引

## 创建

执行 PUT 请求，`/stu` ,即为添加了一个 stu 的索引

```http
PUT http://192.168.80.128:9200/stu
```

## 删除

执行 DELETE 请求 /stu，即为删除索引

```http
DELETE http://192.168.80.128:9200/stu
```

# 文档

## 添加

执行 POST 请求   `/索引/类型/id`  ,设置 Content-Type 为 application/json

```http
POST http://192.168.80.128:9200/stu/student/1001

{
	"sname":"张三",
	"sage":12
}
```

其中 id 可以不设置，会默认生成一个 id 作为唯一标识符

![](img/e1.png)

## 修改

执行 PUT请求，带上 id ，全量覆盖更新,字段变化也会覆盖

```http
PUT http://192.168.80.128:9200/stu/student/1001

{
	"sname":"李四",
	"site":"广州"
}
```

上面每次都有把之前的值先获取到，还可以局部修改

执行 POST ，url 最后带上 `/_update`  作为标识，数据包装在 doc 属性下

```http
POST http://192.168.80.128:9200/stu/student/1001/_update

{
	"doc":{
		"sage":18
	}
}
```

## 删除

执行 DELETE ,带上 id 即可删除

```http
DELETE http://192.168.80.128:9200/stu/student/1001
```

## 查询

执行 GET 请求

```http
GET http://192.168.80.128:9200/stu/student/1001
```

得到 响应的 json 数据

```json
{
    "_index": "stu",
    "_type": "student",
    "_id": "1001",
    "_version": 1,
    "_seq_no": 5,
    "_primary_term": 1,
    "found": true,
    "_source": {
        "sname": "亚索",
        "sage": 12
    }
}
```

全量查询,不指定 id ，通过 `/_search` 标识，但是一次最多返回 10条

```http
GET http://192.168.80.128:9200/stu/student/_search
```

按条件搜索,通过 参数 q=k:v ，设置要查询的参数和值

```http
GET http://192.168.80.128:9200/stu/student/_search?q=sage:12
```

### 分页

通过 size 参数指定查询的个数，from 参数指定从哪个下标开始查，0 为第一个元素下标。

```http
GET http://192.168.80.128:9200/stu/student/_search?size=2&from=2
```



## DSL 查询

通过 POST 请求，把查询条件作为参数，支持复杂的查询条件

```http
POST http://192.168.80.128:9200/stu/student/_search

{
  "query":{
  	"match":{
  		"sage":12
  	}
  }
}
```

### 高亮

查询条件中，设置 highlight 属性，设置高亮字段，高亮字段需要作为查询条件

```json
{
  "query":{
  	"match":{
  		"sname":"亚索"
  	}
  },
  "highlight":{
  	"fields":{
  		"sname":{}
  	}
  }
}
```

得到的单个元素为

```json
{
    "_index": "stu",
    "_type": "student",
    "_id": "1001",
    "_score": 1.3862944,
    "_source": {
        "sname": "亚索",
        "sage": 12
    },
    "highlight": {
        "sname": [
            "<em>亚</em><em>索</em>"
        ]
    }
}
```

### 聚合

聚合类似于 sql 的 group by ，参数中指定要聚合的字段

```json
{
 "aggs":{
 	"all_interesets":{
 		"terms":{
 			"field":"sage"
 		}
 	}
 }
}
```

得到的结果中,就会得到值为该值的有几个,如年龄为12的总共3个

```json
"aggregations": {
    "all_interesets": {
        "doc_count_error_upper_bound": 0,
        "sum_other_doc_count": 0,
        "buckets": [
            {
                "key": 12,
                "doc_count": 3
            },
            {
                "key": 13,
                "doc_count": 2
            }
        ]
    }
}
```

### 批量查询

通过 _mget 作为后缀，参数中指定要查询的 id 数组。

```http
POST http://192.168.80.128:9200/stu/student/_mget

{
	"ids":["1001","Q0ljG3ABPbFlQAd-m6vt","1002"]
}
```

其中一个不存在，不影响返回结果

### 批量添加

通过 _bulk 后缀，请求体的 json 参数为两个对象一组，使用 `\n` 换行作为后缀，最后一行也要加。

第一个对象指定操作类型，和元数据。第二个对象为原始数据 。

```http
POST http://192.168.80.128:9200/stu/student/_bulk

{"create":{"_index":"stu","_type":"student","_id":"2001"}}
{"sname":"盖伦","sage":14}
{"create":{"_index":"stu","_type":"student","_id":"2002"}}
{"sname":"亚索","sage":15}
```

### 批量删除

类似，也是用 _bulk,只需指定元数据即可。

```http
POST http://192.168.80.128:9200/stu/student/_bulk

{"delete":{"_index":"stu","_type":"student","_id":"2001"}}
{"delete":{"_index":"stu","_type":"student","_id":"2002"}}
```

## 元数据

元数据通常使用 `_` 作为前缀

`_index` 索引，相当于数据库

`_type` 文档类型，一类文档需要存放在一起，类似于表

`_id` 一条文档的唯一标识

`_source` 原始数据，发送GET请求时，可以通过参数 `_source` 指定要查询的字段

```http
GET http://192.168.80.128:9200/stu/student/1001?_source=sname,sage
```

得到

```json
{
    "_index": "stu",
    "_type": "student",
    "_id": "1001",
    "_version": 3,
    "_seq_no": 8,
    "_primary_term": 3,
    "found": true,
    "_source": {
        "sname": "张三",
        "sage": 12
    }
}
```

如果作为 url 的一部分，则不包含元数据，只包含原始数据

```http
GET http://192.168.80.128:9200/stu/student/1001/_source
```

响应得到

```json
{
	"sname": "张三",
	"sage": 12
}
```

同样可以带参数，指定需要得到的字段

```http
GET http://192.168.80.128:9200/stu/student/1001/_source?_source=sname
```

`_found` 是否存在，存在则为 true ，否则为 false

判断是否存在还可以发送 HEAD 请求,如果状态码为 200，则存在。404 则不存在

```http
HEAD http://192.168.80.128:9200/stu/student/1001
```

## 映射

相当于定义数据类型

字符串：text 可分词，keyword 不能分词

数字：integer , long , byte , short

小数：double,float

布尔：boolean

日期：date

创建索引时，指定映射

```http
PUT http://192.168.80.128:9200/stu

{
	"mappings":{
		"student":{
			"properties":{
				"sname":{
					"type":"text"
				},
				"sage":{
					"type":"integer"
				},
				"email":{
					"type":"keyword"
				}
			}
		}
	}
}
```

查看映射

```http
GET http://192.168.80.128:9200/stu/_mappings
```

## 高级查询

### term

精确匹配

```http
POST http://192.168.80.128:9200/stu/student/_search

{
	"query":{
		"term":{
			"sage":12
		}
	}
}
```

### terms

与 term 类似，可以匹配多个值

```http
POST http://192.168.80.128:9200/stu/student/_search

{
	"query":{
		"terms":{
			"sage":[12,13]
		}
	}
}
```

### range

查询指定的范围，gt 大于，gte 大于等于，lt 小于，lte 小于等于

```http
POST http://192.168.80.128:9200/stu/student/_search

{
	"query":{
		"range":{
			"sage":{
				"gte":12,
				"lt":16
			}
		}
	}
}
```

### exists

判断是否存在某个字段

```http
POST http://192.168.80.128:9200/stu/student/_search

{
	"query":{
		"exists":{
			"field":"sage"
		}
	}
}
```

### match

模糊匹配

```http
POST http://192.168.80.128:9200/stu/student/_search

{
	"query":{
		"match":{
			"sname":"张"
		}
	}
}
```

### bool

联合查询，将多组条件合并.

must : 必须满足，相当于 与

must_not ：不能满足，相当于 非

should : 可以满足，相当于 或

```http
POST http://192.168.80.128:9200/stu/student/_search

{
	"query":{
		"bool":{
			"must":{
				"match":{
					"sname":"张"
				}
			},
			"must_not":{
				"exists":{
					"field":"site"
				}
			},
			"should":{
				"range":{
					"sage":{
						"gt":13
					}
				}
			}
		}
	}
}
```

## 分词

对字符串进行分词，指定分词器和文本。

```http
POST http://192.168.80.128:9200/_analyze

{
	"analyzer":"standard",
	"text":"hello world"
}
```

由于英文可以使用空格，作为词之间的分割，而中文没有分割符，且不同的分割方法会有不同的语义。

这里需要使用专门的中文分词器插件 ik ，下载地址 <https://github.com/medcl/elasticsearch-analysis-ik/releases> 

进入 elastic目录下的 plugins 目录，注意，下载的版本需和 es 版本一致

```bash
 mkdir ik
 
 把压缩包放入 ik 目录，进入 ik 目录解压
 unzip elasticsearch-analysis-ik-6.8.0.zip
```

最后重启 es，测试，指定分词器为 ik

```http
POST http://192.168.80.128:9200/_analyze

{
	"analyzer":"ik_max_word",
	"text":"我是程序员"
}
```

得到，可以看到分词效果

```json
{
    "tokens": [
        {
            "token": "我",
            "start_offset": 0,
            "end_offset": 1,
            "type": "CN_CHAR",
            "position": 0
        },
        {
            "token": "是",
            "start_offset": 1,
            "end_offset": 2,
            "type": "CN_CHAR",
            "position": 1
        },
        {
            "token": "程序员",
            "start_offset": 2,
            "end_offset": 5,
            "type": "CN_WORD",
            "position": 2
        },
        {
            "token": "程序",
            "start_offset": 2,
            "end_offset": 4,
            "type": "CN_WORD",
            "position": 3
        },
        {
            "token": "员",
            "start_offset": 4,
            "end_offset": 5,
            "type": "CN_CHAR",
            "position": 4
        }
    ]
}
```



